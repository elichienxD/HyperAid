{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperAid\n",
    "\n",
    "Please follow the instruction in the README file for correct running our code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import some basic packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train a hyperbolic embedding model for hierarchical clustering.\"\"\"\n",
    "\n",
    "import argparse\n",
    "from config import config_args\n",
    "from utils.training import add_flags_from_config, get_savedir\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup default arguments\n",
    "\n",
    "You may postpone the choice of `--enc_method,--dec_method,--dataset,--num_nodes,--noise_scale` later.\n",
    "The most important thing to setup here are\n",
    "\n",
    "1) the negative curvature `--c`\n",
    "Here's the choice of `--c` for reproducing our results for all datasets\n",
    "\n",
    "Real-world datasets\n",
    "Zoo: c=200,\n",
    "Iris: c=100,\n",
    "Glass: c=250,\n",
    "Segmentation: c=300,\n",
    "Spambase: c=100.\n",
    "\n",
    "Synthetic datasets\n",
    "c=100\n",
    "\n",
    "2) gpu `--device`\n",
    "Which gpu to use. Set -1 for using cpu.\n",
    "\n",
    "3) `--verbose`\n",
    "Whether to output detail information during training\n",
    "\n",
    "4) `--save`\n",
    "Whether to save results. Set it to be `True` UNLESS you want to fine-tune the model by yourself.\n",
    "Note that in order to use decoders such as T-REX and Ufit, you will need the saved results.\n",
    "The encoder will skip training hyperbolic embeddings for a dataset if some previous result is saved.\n",
    "This will save alot of time and ensure the fairness if you want to test multiple decoders at a time later.\n",
    "Results will be saved in the folder `./embeddings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\"Hyperbolic Hierarchical Clustering.\")\n",
    "parser.add_argument(\"--seed\", type=str, default=0, help=\"model seed to use\")\n",
    "parser.add_argument(\"--enc_method\", type=str, default='Hyp', choices=['Hyp', 'Direct'], help=\"Method for encoding.\")\n",
    "parser.add_argument(\"--dec_method\", type=str, default='random_points', choices=['TreeRep', 'NJ'], help=\"Method for decoding.\")\n",
    "parser.add_argument(\"--dataset\", type=str, default='zoo', help=\"dataset option\")\n",
    "parser.add_argument(\"--num_nodes\", type=int, default=64, help=\"#nodes of synthetic dataset\")\n",
    "parser.add_argument(\"--noise_scale\", type=float, default=0.0, help=\"noise_scale for random_tree synthetic dataset\")\n",
    "parser.add_argument(\"--p\", type=int, default=2, help=\"Lp norm in the cost\")\n",
    "parser.add_argument(\"--c\", type=float, default=1, help=\"negative curvature\") # No use for now.\n",
    "parser.add_argument(\"--burnin\", type=int, default=30, help=\"Burn-in stage epoch.\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=100000, help=\"Batch size. Recommend to set it as large as possible.\") \n",
    "parser.add_argument(\"--dtype\", type=str, default='double', help=\"dtype. Set to double for better precision\")\n",
    "parser.add_argument(\"--scaling_factor\", type=float, default=1, help=\"The scaling factor of all metric.\")\n",
    "parser.add_argument(\"--burnin_factor\", type=float, default=10, help=\"The factor of lr/burnin_lr\")\n",
    "parser.add_argument(\"--verbose\", type=bool, default=True, help=\"To print complete info or not.\")\n",
    "parser.add_argument(\"--dec_repeat\", type=int, default=20, help=\"Number of times to repeat decodeing steps.\")\n",
    "parser.add_argument(\"--check_tree\", type=bool, default=False, help=\"Check if the (best) output is a tree metric.\")\n",
    "parser.add_argument(\"--eval_every\", type=int, default=1)\n",
    "parser.add_argument(\"--patience\", type=int, default=50)\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=0.001)\n",
    "parser.add_argument(\"--epochs\", type=int, default=500)\n",
    "parser.add_argument(\"--rank\", type=int, default=2)\n",
    "parser.add_argument(\"--Normalization\", type=bool, default=False)\n",
    "parser.add_argument(\"--init_size\", type=float, default=1e-6)\n",
    "parser.add_argument(\"--device\", type=int, default='0', help=\"-1 for cpu.\")\n",
    "\n",
    "parser.add_argument(\"--anneal_every\", type=int, default=200)\n",
    "parser.add_argument(\"--anneal_factor\", type=float, default=1.0)\n",
    "parser.add_argument(\"--save\", type=bool, default=True)\n",
    "parser.add_argument(\"--num_workers\", type=int, default=1)\n",
    "# Load other args from default config.\n",
    "parser = add_flags_from_config(parser, config_args)\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "# We use os.environ['CURVATURE'] to pass the args.curvature to util.poincare.py!\n",
    "os.environ['CURVATURE']=str(args.c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import the rest packages\n",
    "\n",
    "Remember to check whether the negative curvature is consistant with your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current curvature is:-1.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import ipdb\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import optim\n",
    "from datasets.hc_dataset import MetricDataset\n",
    "from datasets.loading import load_data\n",
    "from model.hyphc import MetricHypHC\n",
    "from utils.metrics import dasgupta_cost\n",
    "from datasets.triples import generate_all_pairs\n",
    "\n",
    "import itertools\n",
    "from scipy.optimize import linprog\n",
    "import networkx as nx\n",
    "import matplotlib as plt\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "from scipy.sparse.csgraph import shortest_path\n",
    "from scipy.cluster.hierarchy import linkage as Linkage\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "import scipy.spatial.distance as ssd\n",
    "import newick\n",
    "\n",
    "os.environ['DATAPATH']=\"./data\"\n",
    "os.environ['SAVEPATH']=\"./embeddings\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Import Julia, also the NJ and TreeRep methods\n",
    "\n",
    "It may take a while (2 minutes in my case)\n",
    "\n",
    "Also please set `jpath` as the path to julia you install in the conda enviroment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for importing: 136.43162775039673\n"
     ]
    }
   ],
   "source": [
    "###### import setting for TreeRep\n",
    "\n",
    "# # # This allows multithreading for julia\n",
    "# # # use 16 thread. Note that using more than 16 thread will incur errors due to the TreeRep implementation.\n",
    "# os.environ[\"JULIA_NUM_THREADS\"] = str(16)\n",
    "\n",
    "from julia.api import Julia\n",
    "jpath = \"YOUR_PATH/miniconda3/envs/HyperAid/bin/julia\" # path to Julia, from current directory (your path may be slightly different)\n",
    "jl = Julia(runtime=jpath, compiled_modules=False) # compiled_modules=True may work for you; it didn't for me\n",
    "\n",
    "t = time.time()\n",
    "import TreeRep_myVer.src.TreeRepy as TreeRepy\n",
    "import TreeRep_myVer.src.NJpy as NJpy\n",
    "print(\"Time for importing:\",time.time() - t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Include our own functions (Utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def newick2W(trees,n):\n",
    "    # This convert a tree from newick format to distance csr matrix, including interior nodes ((2n-1)-by-(2n-1))\n",
    "    # Note that all internal nodes are named \"None\"\n",
    "    # All leaves node have name in np.arange(n)\n",
    "    \n",
    "    W = csr_matrix(((2*n-1),(2*n-1)))\n",
    "    assert len(trees[0].descendants) == 2 # An internal/root node should always have 2 children!\n",
    "    \n",
    "    W,_,_ = Fill_W_from_tree(trees[0],W,n)\n",
    "    return W\n",
    "\n",
    "def Fill_W_from_tree(subtree,W,cur_idx):   \n",
    "    # Check if we visit current node\n",
    "    if subtree.name == None:\n",
    "        # Not visit yet, give id\n",
    "        subtree.name = str(cur_idx)\n",
    "        cur_idx += 1\n",
    "    # Now add all adjacent relation to W\n",
    "    if not subtree.is_leaf:\n",
    "        Flag = False\n",
    "        for i in range(len(subtree.descendants)):\n",
    "            if subtree.descendants[i].name == None:\n",
    "                subtree.descendants[i].name = str(cur_idx)\n",
    "                cur_idx += 1\n",
    "                Flag = True\n",
    "\n",
    "            W[int(subtree.descendants[i].name),int(subtree.name)] = subtree.descendants[i].length\n",
    "            W[int(subtree.name),int(subtree.descendants[i].name)] = subtree.descendants[i].length\n",
    "\n",
    "            if Flag:\n",
    "                W,subtree.descendants[i],cur_idx = Fill_W_from_tree(subtree.descendants[i],W,cur_idx)\n",
    "    \n",
    "    return W,subtree,cur_idx\n",
    "\n",
    "\n",
    "def generate_rand_TreeMetric_V2(N,option='unweight',noise_ratio=0.0):\n",
    "    # Generate (weighted) random binary tree in newick format.\n",
    "    T = generate_tree_newick(np.arange(N),option=option)\n",
    "    # Make it into a tree data structure in python\n",
    "    T = newick.loads(T)\n",
    "    # Get the distance matrix for leaves in T\n",
    "    W = newick2W(T,N)\n",
    "    \n",
    "    # Add noise. We randomly add noise_ratio*(2n-2) fake connections between nodes (including internal nodes).\n",
    "    m_Noise = int(noise_ratio*(2*N-2))\n",
    "    for _ in range(m_Noise):\n",
    "        idx0,idx1 = np.random.choice(N,size=(2),replace=False)\n",
    "        if option == 'unweight':\n",
    "            W[idx0,idx1] = 1.0\n",
    "            W[idx1,idx0] = 1.0\n",
    "        else:\n",
    "            W[idx0,idx1] = np.random.random()\n",
    "            W[idx1,idx0] = np.random.random()\n",
    "    \n",
    "    # Now construct the shortest path based on this noisy observation\n",
    "    Noisy_D = get_leaves_Dmatrix(W,N)\n",
    "    \n",
    "    return Noisy_D\n",
    "\n",
    "\n",
    "def generate_tree_newick(L,option='unweight',balance=False):\n",
    "    # base case\n",
    "    if len(L) == 1: \n",
    "        return str(L[0])\n",
    "    else:\n",
    "        perm_L = np.random.permutation(L)\n",
    "        if balance:\n",
    "            split = int(len(L)/2)\n",
    "        else:\n",
    "            split = np.random.randint(1,len(L))\n",
    "        left = perm_L[:split]\n",
    "        right = perm_L[split:]\n",
    "    # recursion\n",
    "    if option == 'unweight':\n",
    "        return str((generate_tree_newick(left,balance=balance)+':1.0',\n",
    "                    generate_tree_newick(right,balance=balance)+':1.0')).replace(' ','').replace(\"'\",\"\")\n",
    "    else:\n",
    "        left_weight = ':{0:.5f}'.format(np.random.rand())\n",
    "        right_weight = ':{0:.5f}'.format(np.random.rand())\n",
    "        return str((generate_tree_newick(left,option=option,balance=balance)+left_weight,\n",
    "                    generate_tree_newick(right,option=option,balance=balance)+right_weight)).replace(' ','').replace(\"'\",\"\")\n",
    "\n",
    "def newick2dist(trees,n):\n",
    "    # This convert a tree from newick format to distance matrix (n-by-n)\n",
    "    # Note that all internal nodes are named \"None\"\n",
    "    # All leaves node have name in np.arange(n)\n",
    "    \n",
    "    D = np.zeros((n,n))\n",
    "    assert len(trees[0].descendants) == 2 # An internal/root node should always have 2 children!\n",
    "    \n",
    "    \n",
    "    D = Fill_D_from_tree(trees[0],D)\n",
    "    return D\n",
    "\n",
    "def Fill_D_from_tree(subtree,D):\n",
    "    All_leaves = np.arange(D.shape[0])    \n",
    "\n",
    "    num_children = len(subtree.descendants)\n",
    "    for i in range(num_children):\n",
    "        sub_leaves = np.array(subtree.descendants[i].get_leaf_names()).astype(int)\n",
    "        # We simply set the negative branch weights to zero following the common practice\n",
    "        if subtree.descendants[i].length>0:\n",
    "            D[np.ix_(sub_leaves,np.setdiff1d(All_leaves,sub_leaves))] += subtree.descendants[i].length\n",
    "            D[np.ix_(np.setdiff1d(All_leaves,sub_leaves),sub_leaves)] += subtree.descendants[i].length\n",
    "    \n",
    "    \n",
    "        if subtree.descendants[i].name == None:\n",
    "            D = Fill_D_from_tree(subtree.descendants[i],D)\n",
    "    \n",
    "    return D\n",
    "\n",
    "def sim2metric(S):\n",
    "    \"\"\"\n",
    "    Input S is a n-by-n np array, representing similarities.\n",
    "    Output D turn S into a metric.\n",
    "    It should satisfy:\n",
    "    1) Sij = 1 iff Dij = 0\n",
    "    2) Sij = -1 iff Dij = max(D)\n",
    "    3) basic metric properties (triangle ineq...etc)\n",
    "    \n",
    "    Now we choose D = 1-S\n",
    "    \"\"\"\n",
    "    return 1-S\n",
    "\n",
    "def generate_tree(L):\n",
    "    # base case\n",
    "    if len(L) == 1: \n",
    "        return L[0]\n",
    "    else:\n",
    "        perm_L = np.random.permutation(L)\n",
    "        split = np.random.randint(1,len(L))\n",
    "        left = perm_L[:split]\n",
    "        right = perm_L[split:]\n",
    "    # recursion\n",
    "    return (generate_tree(left), generate_tree(right))\n",
    "\n",
    "def plot_rand_binary_tree(brt,n,plot=True,normalization=False,d_mtx=None):\n",
    "    w_star = None\n",
    "    A = np.zeros((2*n-1,2*n-1))\n",
    "    # Note that we keep the leave node id the same!!!\n",
    "    # Thus the internal node id start at n.\n",
    "    # For clarity, we set node id n to be the root node\n",
    "    A,next_idx = Fill_adj(brt,n,A)\n",
    "    \n",
    "    assert next_idx == 2*n-1\n",
    "    \n",
    "    if normalization:\n",
    "        # Apply Puoya's normalized weight\n",
    "        A_T=np.zeros((n*(n-1)/2,2*n-2))\n",
    "        # First construct an edge name dictionary\n",
    "        w_dict = -np.ones((2*n-1,2*n-1))\n",
    "        w_dict,next_idx,cur_w_idx = Fill_w_dict(brt,n,w_dict)\n",
    "        assert next_idx == 2*n-1\n",
    "        assert cur_w_idx == 2*n-2\n",
    "        \n",
    "        # Now go through all shortest path for leaves and get d_vec\n",
    "        G = nx.from_numpy_matrix(np.matrix(A))\n",
    "        All_path = nx.shortest_path(G)\n",
    "        count = 0\n",
    "        d_vec = np.zeros(n*(n-1)/2)\n",
    "        for i in range(n):\n",
    "            for j in range(i+1,n):\n",
    "                d_vec[count] = d_mtx[i,j]\n",
    "                path = All_path[i][j]\n",
    "                for k in range(len(path)-1):\n",
    "                    A_T[count,int(w_dict[path[k],path[k+1]])] = 1\n",
    "                count += 1\n",
    "        \n",
    "        # optimal weight\n",
    "        w_star = np.matmul(np.matmul(np.linalg.pinv(np.matmul(A_T.transpose(),A_T)),A_T.transpose()),d_vec)\n",
    "        # modify adj matrix A\n",
    "        for i in range(2*n-1):\n",
    "            for j in range(i+1,2*n-1):\n",
    "                if A[i,j] == 1:\n",
    "                    A[i,j] = w_star[int(w_dict[i,j])]\n",
    "                    A[j,i] = w_star[int(w_dict[i,j])]\n",
    "    \n",
    "    # Compute d, the shortest path for leave nodes\n",
    "    G = nx.from_numpy_matrix(np.matrix(A))\n",
    "    d = nx.algorithms.shortest_paths.dense.floyd_warshall_numpy(G)\n",
    "    d = d[:n]\n",
    "    d = d[:,:n]\n",
    "    \n",
    "    if plot:\n",
    "        plot_Tree_metric(A,n,n)\n",
    "    \n",
    "    return d,w_star\n",
    "\n",
    "def Fill_adj(brt,cur_idx,A):\n",
    "    left = brt[0]\n",
    "    if isinstance(left,np.int64):\n",
    "        A[cur_idx,left] = 1\n",
    "        A[left,cur_idx] = 1\n",
    "        next_idx = cur_idx+1\n",
    "    else:\n",
    "        cur_idx_left = cur_idx+1\n",
    "        A[cur_idx,cur_idx_left] = 1\n",
    "        A[cur_idx_left,cur_idx] = 1\n",
    "        A,next_idx = Fill_adj(left,cur_idx_left,A)\n",
    "    \n",
    "    right = brt[1]\n",
    "    if isinstance(right,np.int64):\n",
    "        A[cur_idx,right] = 1\n",
    "        A[right,cur_idx] = 1\n",
    "    else:\n",
    "        cur_idx_right = next_idx\n",
    "        A[cur_idx,cur_idx_right] = 1\n",
    "        A[cur_idx_right,cur_idx] = 1\n",
    "        A,next_idx = Fill_adj(right,cur_idx_right,A)\n",
    "\n",
    "    return A,next_idx\n",
    "    \n",
    "def Fill_w_dict(brt,cur_idx,w_dict,cur_w_idx=0):\n",
    "    left = brt[0]\n",
    "    if isinstance(left,np.int64):\n",
    "        w_dict[cur_idx,left] = cur_w_idx\n",
    "        w_dict[left,cur_idx] = cur_w_idx\n",
    "        next_idx = cur_idx+1\n",
    "        cur_w_idx += 1\n",
    "    else:\n",
    "        cur_idx_left = cur_idx+1\n",
    "        w_dict[cur_idx,cur_idx_left] = cur_w_idx\n",
    "        w_dict[cur_idx_left,cur_idx] = cur_w_idx\n",
    "        cur_w_idx += 1\n",
    "        w_dict,next_idx,cur_w_idx = Fill_w_dict(left,cur_idx_left,w_dict,cur_w_idx)\n",
    "    \n",
    "    right = brt[1]\n",
    "    if isinstance(right,np.int64):\n",
    "        w_dict[cur_idx,right] = cur_w_idx\n",
    "        w_dict[right,cur_idx] = cur_w_idx\n",
    "        cur_w_idx += 1\n",
    "    else:\n",
    "        cur_idx_right = next_idx\n",
    "        w_dict[cur_idx,cur_idx_right] = cur_w_idx\n",
    "        w_dict[cur_idx_right,cur_idx] = cur_w_idx\n",
    "        cur_w_idx += 1\n",
    "        w_dict,next_idx,cur_w_idx = Fill_w_dict(right,cur_idx_right,w_dict,cur_w_idx)\n",
    "\n",
    "    return w_dict,next_idx,cur_w_idx\n",
    "\n",
    "def plot_Tree_metric(W,N_leaves,root_id=-1,Return_metric=False,plot=True,option='scipy'):\n",
    "    \n",
    "    print('Start computing shortest path')\n",
    "    \n",
    "    # First to remove potential zero rows\n",
    "    W = W[~np.all(W == 0, axis=1)]\n",
    "    W = W[:,~np.all(W == 0, axis=0)]\n",
    "    \n",
    "#     # Replace all negative entries to 0 (if any)\n",
    "#     W[W<0] = 0.0\n",
    "    \n",
    "#     # Rounding for better vitualization\n",
    "#     W = np.around(W,3)\n",
    "    \n",
    "    G = nx.from_numpy_matrix(np.matrix(W))\n",
    "    \n",
    "    if plot:\n",
    "        # Use spring_layout to handle positioning of graph\n",
    "        layout = nx.kamada_kawai_layout(G)\n",
    "\n",
    "        # # Use a list for node_sizes\n",
    "        # sizes = [1000,400,200]\n",
    "\n",
    "        # # Use a list for node colours\n",
    "        # # Here we use different color to distiguish leave nodes and steiner nodes\n",
    "        # # By default, we use 'tab:blue' for leave nodes and 'tab:red' for steiner nodes\n",
    "        N_all = W.shape[0]\n",
    "        color_map = []\n",
    "        for _ in range(N_leaves):\n",
    "            color_map.append('tab:blue')\n",
    "        for _ in range(N_all-N_leaves):\n",
    "            color_map.append('tab:red')\n",
    "\n",
    "        if root_id>-1:\n",
    "            color_map[root_id] = 'tab:green'\n",
    "\n",
    "        # Draw the graph using the layout - with_labels=True if you want node labels.\n",
    "        nx.draw(G, layout, with_labels=True, node_color= color_map)\n",
    "\n",
    "        # Get weights of each edge and assign to labels\n",
    "        labels = nx.get_edge_attributes(G, \"weight\")\n",
    "\n",
    "        # Draw edge labels using layout and list of labels\n",
    "        nx.draw_networkx_edge_labels(G, pos=layout, edge_labels=labels)\n",
    "    \n",
    "    # Also compute the shortest distance matrix for leaves\n",
    "    if Return_metric:\n",
    "        if option == 'networkx':\n",
    "            n = N_leaves\n",
    "            d = nx.algorithms.shortest_paths.dense.floyd_warshall_numpy(G)\n",
    "            d = d[:n]\n",
    "            d = d[:,:n]\n",
    "            return np.array(d)\n",
    "        elif option == 'scipy':\n",
    "            n = N_leaves\n",
    "            graph = csr_matrix(W) \n",
    "            d = shortest_path(csgraph=graph,directed=False)\n",
    "            d = d[:n]\n",
    "            d = d[:,:n]\n",
    "            return d\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_leaves_Dmatrix(W,N_leaves):\n",
    "    # Input W is a csr_matrix.\n",
    "    n = N_leaves \n",
    "    d = shortest_path(csgraph=W,directed=False)\n",
    "    d = d[:n]\n",
    "    d = d[:,:n]\n",
    "    return d\n",
    "\n",
    "def Linkage2dist(Z):\n",
    "    # First, build a dictionary for mapping all nodes to actual cluters\n",
    "    n = Z.shape[0]+1\n",
    "    # Include all leaves as singleton first\n",
    "    W_Size = int(max(Z[:,0].max(),Z[:,1].max()))+2\n",
    "    W = csr_matrix((W_Size,W_Size))    \n",
    "\n",
    "    for i in range(Z.shape[0]):\n",
    "        idx0 = Z[i,0]\n",
    "        idx1 = Z[i,1]\n",
    "        length = Z[i,2]\n",
    "        pid = i+n\n",
    "        W[idx0,pid] = length\n",
    "        W[idx1,pid] = length\n",
    "        W[pid,idx0] = length\n",
    "        W[pid,idx1] = length\n",
    "\n",
    "    d = shortest_path(csgraph=W,directed=False)\n",
    "    d = d[:n]\n",
    "    d = d[:,:n]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Include decoder wrapper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Decoding_TreeRep(D_input,D_target,p=2,repeat=20,verbose=False,rng_seed=0,tol=1e-6):\n",
    "    \"\"\"\n",
    "    Since TreeRep is a randomized algorithm, when D is not a tree metric, the output tree metric is random.\n",
    "    Hence we repeat the decoding step using TreeRep with $repeat runs.\n",
    "    We return the mean loss, std, best loss and the corresponding tree metric\n",
    "    \n",
    "    Set verbose = True for printing out more info such as running time...etc.\n",
    "    \n",
    "    Note that somehow direct decoding original metric has chance to incur tree weight = inf.\n",
    "    Now we simply ignore that run and restart.\n",
    "    \n",
    "    Also note that we pass and use random seed from rng_seed*repeat to rng_seed*repeat+(repeat-1). \n",
    "    \"\"\"\n",
    "    # Rounding D_input to prevent numerical issue! tol should be smaller than the min dif of entries in D_input!\n",
    "    D_input = np.around(D_input,5)\n",
    "    \n",
    "    Results = np.zeros(repeat)\n",
    "    Best_loss = np.inf\n",
    "    rp = 0\n",
    "    Flag = True\n",
    "    BAD_time = 0\n",
    "    BAD_MAX = repeat # prevent infinte loop!\n",
    "    cur_seed = rng_seed*repeat\n",
    "    with tqdm(total=repeat) as bar:\n",
    "        while Flag:\n",
    "            t = time.time()\n",
    "            D_input_tree = TreeRepy.TreeRep(D_input,rng_seed=cur_seed)\n",
    "#             # Remember to remove 0 rows/cols\n",
    "#             D_input_tree = D_input_tree[~np.all(D_input_tree == 0, axis=1)]\n",
    "#             D_input_tree = D_input_tree[:,~np.all(D_input_tree == 0, axis=0)]\n",
    "            \n",
    "            #Check if D_input_tree has negative entries\n",
    "            if D_input_tree[D_input_tree<0].shape[1]>0:\n",
    "                print('TreeRep error: decoding tree contains negative weights!')\n",
    "                BAD_time += 1\n",
    "                # Remember to skip this random seed...\n",
    "                cur_seed += 1\n",
    "                if BAD_time>=BAD_MAX:\n",
    "                    return -1, -1, -1, -1, BAD_time\n",
    "                continue\n",
    "\n",
    "            # Plot tree decoded from D_metric\n",
    "            D_input_tree_metric = get_leaves_Dmatrix(D_input_tree,D_input.shape[0])\n",
    "\n",
    "            # Check if decoding tree has inf/nan\n",
    "            if np.isinf(D_input_tree_metric).any() or np.isnan(D_input_tree_metric).any():\n",
    "                if verbose:\n",
    "                    print('TreeRep error: decoding tree metric contains inf/nan weight!')\n",
    "                BAD_time += 1\n",
    "                # Remember to skip this random seed...\n",
    "                cur_seed += 1\n",
    "                if BAD_time>=BAD_MAX:\n",
    "                    return -1, -1, -1, -1, BAD_time\n",
    "                continue\n",
    "\n",
    "            if verbose:\n",
    "                # Check if there's inf\n",
    "                print(np.isinf(D_input_tree_metric).any())\n",
    "                print(D_input_tree_metric)\n",
    "\n",
    "            loss = (np.abs(D_target-D_input_tree_metric)**p).sum().sum()**(1/p)\n",
    "            Results[rp] = loss\n",
    "\n",
    "            if loss < Best_loss:\n",
    "                D_output = D_input_tree_metric\n",
    "                \n",
    "            rp += 1\n",
    "            cur_seed += 1\n",
    "            bar.update(1)\n",
    "            if rp >= repeat:\n",
    "                Flag = False\n",
    "    \n",
    "    return Results.mean(), Results.std(), Results.min(), D_output, BAD_time\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "def Decoding_NJ(D_input,D_target,p=2,verbose=False):\n",
    "    D_input_tree = NJpy.NJ(D_input)\n",
    "    \n",
    "    #Check if D_input_tree has negative entries\n",
    "    if D_input_tree[D_input_tree<0].shape[1]>0:\n",
    "        print('NJ error: decoding tree contains negative weight!')\n",
    "        return -1, -1, -1, -1, 1\n",
    "        \n",
    "    D_output = get_leaves_Dmatrix(D_input_tree,D_target.shape[0])\n",
    "\n",
    "    if np.isinf(D_output).any():\n",
    "        print('NJ error: decoding tree metric contains inf!')\n",
    "        return -1, -1, -1, -1, 1\n",
    "    \n",
    "    \n",
    "    loss = (np.abs(D_target-D_output)**p).sum().sum()**(1/p)\n",
    "    \n",
    "    return loss, -1, loss, D_output, 0\n",
    "\n",
    "def Decoding_linkage(D_input,D_target,p=2,verbose=False,link_method='single'):\n",
    "    \"\"\"\n",
    "    This decoding option using linkage based method as decoder.\n",
    "    Note that the output is a *ultrametric* instead of a general *tree (additive) metric*!\n",
    "    We mainly use scipy.cluster.hierarchy.linkage.\n",
    "    The link_method correspond to the method option in scipy.cluster.hierarchy.linkage.\n",
    "    \n",
    "    Warning! If D_input is *NOT* Euclidean, then do not use ‘centroid’, ‘median’, and ‘ward’ method!\n",
    "    See scipy website for more information: https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    Z = Linkage(ssd.squareform(D_input),method=link_method)\n",
    "    # Check if Z has negative edge weights\n",
    "    if Z[:,2].squeeze()[Z[:,2].squeeze()<0]:\n",
    "        print('Linkage error: decoding tree contains negative weight!')\n",
    "        return -1, -1, -1, -1, 1\n",
    "    \n",
    "    D_output = ssd.squareform(cophenet(Z))\n",
    "    # Check if D_output has inf/nan weights\n",
    "    if np.isinf(D_output).any() or np.isnan(D_output).any():\n",
    "        print('Linkage error: decoding tree metric contains inf/nan weight!')\n",
    "        return -1, -1, -1, -1, 1\n",
    "    \n",
    "    loss = (np.abs(D_target-D_output)**p).sum().sum()**(1/p)\n",
    "    \n",
    "    return loss, -1, loss, D_output, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Include encoder wrapper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # set seed\n",
    "    logging.info(\"Using seed {}.\".format(args.seed))\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "    # set precision\n",
    "    logging.info(\"Using {} precision.\".format(args.dtype))\n",
    "    if args.dtype == \"double\":\n",
    "        torch.set_default_dtype(torch.float64)\n",
    "\n",
    "    # create dataset\n",
    "    if args.dataset in ['zoo','iris','glass','segmentation','spambase']:\n",
    "        x, y_true, similarities = load_data(args.dataset)\n",
    "        D_metric = sim2metric(similarities)\n",
    "        D_metric *= args.scaling_factor\n",
    "    elif args.dataset in ['random_tree']:\n",
    "        D_metric = generate_rand_TreeMetric_V2(args.num_nodes,option='unweight',noise_ratio=args.noise_scale)\n",
    "        D_metric *= args.scaling_factor\n",
    "        y_true = np.zeros(D_metric.shape[0],int)\n",
    "    else:\n",
    "        print('No such dataset option! args.dataset:{}'.format(args.dataset))\n",
    "        raise \n",
    "    \n",
    "    # Return D_metric directly if arg.enc_method == 'Direct' (no encoding step)\n",
    "    if args.enc_method == 'Direct':\n",
    "        return np.array(D_metric)/args.scaling_factor\n",
    "    \n",
    "    # get saving directory\n",
    "    if args.save:\n",
    "#         save_dir = get_savedir(args)\n",
    "        if args.dataset in ['zoo','iris','glass','segmentation','spambase']:\n",
    "            save_dir = os.path.join(os.environ[\"SAVEPATH\"],\n",
    "                                args.dataset, 'seed_{}_curv_{}'.format(args.seed,os.environ[\"CURVATURE\"]))\n",
    "        else:\n",
    "            save_dir = os.path.join(os.environ[\"SAVEPATH\"],\n",
    "                                args.dataset,'nodes_{}_noise_{}'.format(args.num_nodes,args.noise_scale),\n",
    "                                'seed_{}_curv_{}'.format(args.seed,os.environ[\"CURVATURE\"]))\n",
    "        logging.info(\"Save directory: \" + save_dir)\n",
    "        save_path = os.path.join(save_dir, \"model_{}.pkl\".format(args.seed))\n",
    "        save_Dpath = os.path.join(save_dir, \"D_metric_{}_{}.npy\".format(args.dataset,args.seed))\n",
    "        if os.path.exists(save_dir):\n",
    "            if os.path.exists(save_Dpath) and (args.enc_method != 'Direct'):\n",
    "                logging.info(\"D_metric with the same configuration parameters already exists.\")\n",
    "                logging.info(\"Load and return existing D_metric\")\n",
    "                \n",
    "                return np.array(D_metric)/args.scaling_factor, np.load(save_Dpath), -1\n",
    "            if os.path.exists(save_path):\n",
    "                logging.info(\"Model with the same configuration parameters already exists.\")\n",
    "                logging.info(\"Exiting. Just generate D_metric from it!\")\n",
    "                return\n",
    "        else:\n",
    "            os.makedirs(save_dir)\n",
    "            with open(os.path.join(save_dir, \"config.json\"), 'w') as fp:\n",
    "                json.dump(args.__dict__, fp)\n",
    "        log_path = os.path.join(save_dir, \"train_{}.log\".format(args.seed))\n",
    "        hdlr = logging.FileHandler(log_path)\n",
    "        formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "        hdlr.setFormatter(formatter)\n",
    "        logger.addHandler(hdlr)\n",
    "    \n",
    "    dataset = MetricDataset(D_metric, labels=y_true, num_samples=args.num_samples)\n",
    "    dataloader = data.DataLoader(dataset, batch_size=args.batch_size, shuffle=True,\n",
    "                                 num_workers=args.num_workers, pin_memory=True)\n",
    "\n",
    "    # create model\n",
    "    model = MetricHypHC(dataset.n_nodes, args.rank, args.temperature,\n",
    "                        args.init_size, args.max_scale, scaling_factor=args.scaling_factor)\n",
    "    \n",
    "    if args.device == -1:\n",
    "        device = torch.device('cpu')\n",
    "    else:\n",
    "        device = torch.device('cuda:{}'.format(args.device))\n",
    "    model.to(device)\n",
    "    model.all_pairs = model.all_pairs.to(device)\n",
    "    \n",
    "    \n",
    "    # create optimizer\n",
    "    Optimizer = getattr(optim, args.optimizer)\n",
    "    optimizer = Optimizer(model.parameters(), args.learning_rate)\n",
    "    # train model\n",
    "    best_cost = np.inf\n",
    "    best_model = None\n",
    "    counter = 0\n",
    "    logging.info(\"Start training\")\n",
    "    for epoch in range(args.epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        # burn-in (assuming input lr is for burn-in stage)\n",
    "        if epoch==args.burnin:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] *= args.burnin_factor\n",
    "                lr = param_group['lr']\n",
    "            logging.info(\"Burn-in stage end. Normal learning rate to: {}\".format(lr))\n",
    "            \n",
    "        if args.verbose:\n",
    "            with tqdm(total=len(dataloader), unit='ex') as bar:\n",
    "                for step, (pair_ids, pair_D_metrics) in enumerate(dataloader):\n",
    "                    pair_ids = pair_ids.to(device)\n",
    "                    pair_D_metrics = pair_D_metrics.to(device)\n",
    "                    loss = model.loss(pair_ids, pair_D_metrics,p=args.p,Normalization=args.Normalization)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    bar.update(1)\n",
    "                    bar.set_postfix(loss=f'{loss.item():.6f}')\n",
    "                    total_loss += loss\n",
    "        else:\n",
    "            for step, (pair_ids, pair_D_metrics) in enumerate(dataloader):\n",
    "                pair_ids = pair_ids.to(device)\n",
    "                pair_D_metrics = pair_D_metrics.to(device)\n",
    "                loss = model.loss(pair_ids, pair_D_metrics,p=args.p,Normalization=args.Normalization)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss\n",
    "        \n",
    "        total_loss = total_loss.item()**(1/args.p) / (step + 1.0)\n",
    "        \n",
    "        if args.verbose:\n",
    "            logging.info(\"\\t Epoch {} | average train loss: {:.6f}\".format(epoch, total_loss))\n",
    "\n",
    "        # keep best embeddings\n",
    "        if (epoch + 1) % args.eval_every == 0:\n",
    "            model.eval()\n",
    "            cost = 0.0\n",
    "            for step, (pair_ids, pair_D_metrics) in enumerate(dataloader):\n",
    "                pair_ids = pair_ids.to(device)\n",
    "                pair_D_metrics = pair_D_metrics.to(device)\n",
    "                loss = model.loss(pair_ids, pair_D_metrics,p=args.p,Normalization=args.Normalization)\n",
    "                cost += loss\n",
    "            cost = cost.item()**(1/args.p)\n",
    "            if args.verbose:\n",
    "                logging.info(\"{}:\\t{:.4f}\".format(\"Lp norm cost\", cost))\n",
    "            if cost < best_cost:\n",
    "                counter = 0\n",
    "                best_cost = cost\n",
    "                best_model = model.state_dict()\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter == args.patience:\n",
    "                    if args.verbose:\n",
    "                        logging.info(\"Early stopping.\")\n",
    "                    break\n",
    "        \n",
    "        # anneal temperature\n",
    "        if (epoch + 1) % args.anneal_every == 0:\n",
    "            model.anneal_temperature(args.anneal_factor)\n",
    "            if args.verbose:\n",
    "                logging.info(\"Annealing temperature to: {}\".format(model.temperature))\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] *= args.anneal_factor\n",
    "                lr = param_group['lr']\n",
    "            if args.verbose:\n",
    "                logging.info(\"Annealing learning rate to: {}\".format(lr))\n",
    "    \n",
    "    \n",
    "    logging.info(\"Optimization finished.\")\n",
    "    if best_model is not None:\n",
    "        # load best model\n",
    "        model.load_state_dict(best_model)\n",
    "\n",
    "    if args.save:\n",
    "        # save best embeddings\n",
    "        logging.info(\"Saving best model at {}\".format(save_path))\n",
    "        torch.save(best_model, save_path)\n",
    "    \n",
    "    # evaluation\n",
    "    model.eval()\n",
    "#     logging.info(\"Decoding embeddings (TODO).\")\n",
    "#     tree = model.decode_tree(fast_decoding=args.fast_decoding)\n",
    "    logging.info(\"Compute Lp norm cost.\")\n",
    "    D_hyp = model.get_D_hyp(Normalization=args.Normalization).cpu()\n",
    "    cost = torch.sum(torch.sum((torch.abs(torch.from_numpy(D_metric)-D_hyp)/args.scaling_factor)**args.p)/2)**(1/args.p)\n",
    "    logging.info(\"{}:\\t{:.4f}\".format(\"Lp norm cost\", cost))\n",
    "    \n",
    "    \n",
    "    \n",
    "    if args.save and (args.enc_method != 'Direct'):\n",
    "        # save resulting distance matrix\n",
    "        logging.info(\"Saving D_hyp at {}\".format(save_Dpath))\n",
    "        np.save(save_Dpath,D_hyp.detach().numpy()/args.scaling_factor)\n",
    "    \n",
    "    if args.save:\n",
    "        logger.removeHandler(hdlr)\n",
    "        \n",
    "    return np.array(D_metric)/args.scaling_factor, D_hyp.detach().numpy()/args.scaling_factor, model.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Wrapping for experiments\n",
    "\n",
    "Note that all the hyperparameters for each tested datasets in our paper are included in this part.\n",
    "If `args.save=True`, a summarized csv file will be saved at `./Results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def One_Run_Exp(args):\n",
    "    REPEAT = args.dec_repeat\n",
    "    \n",
    "    if args.enc_method in ['Hyp']:\n",
    "        # Encoding part\n",
    "        start = time.time()\n",
    "        D_metric,D_hyp, model = train(args)\n",
    "        Encode_time = time.time()-start\n",
    "        \n",
    "        # Decoding part\n",
    "        if args.dec_method in ['TreeRep']:\n",
    "            start = time.time()\n",
    "            D_hyp_results = Decoding_TreeRep(D_input=D_hyp,D_target=D_metric,p=args.p,repeat=args.dec_repeat,verbose=False,rng_seed=args.seed)\n",
    "            D_hyp_decode_time = (time.time()-start)/args.dec_repeat\n",
    "        elif args.dec_method in ['NJ']:\n",
    "            start = time.time()\n",
    "            D_hyp_results = Decoding_NJ(D_input=D_hyp,D_target=D_metric,p=args.p,verbose=False)\n",
    "            D_hyp_decode_time = (time.time()-start)\n",
    "        elif args.dec_method in ['single','complete','average','weighted']:\n",
    "            start = time.time()\n",
    "            D_hyp_results = Decoding_linkage(D_input=D_hyp,D_target=D_metric,p=args.p,\n",
    "                                             verbose=False,link_method=args.dec_method)\n",
    "            D_hyp_decode_time = (time.time()-start)\n",
    "        elif args.dec_method in ['centroid','median','ward']:\n",
    "            print('These method do not support non-Euclidean metric input!')\n",
    "            D_hyp_results = [-1,-1,-1,-1,-1]\n",
    "            D_hyp_decode_time = -1\n",
    "        else:\n",
    "            raise NameError('No such a decoding method! name={}'.format(args.dec_method))\n",
    "        \n",
    "        if args.check_tree:\n",
    "            print(is_treemetric(D_hyp_results[3]))\n",
    "        \n",
    "        \n",
    "        # Results\n",
    "        emb_loss = ((D_metric-D_hyp)**args.p).sum().sum()**(1/args.p)\n",
    "        \n",
    "        if args.verbose:\n",
    "            print(\"Seed:{},p={},dataset:{},num_nodes={}\".format(args.seed,args.p,args.dataset,D_hyp.shape[0]))\n",
    "            print(\"Enc:{},Dec:{}\".format(args.enc_method,args.dec_method))\n",
    "            print(\"Original embedding loss:{}\".format(emb_loss))\n",
    "            print(\"Tree loss:              {}±{}, best loss:{}\".format(np.around(D_hyp_results[0],4),np.around(D_hyp_results[1],4),np.around(D_hyp_results[2],4)))\n",
    "            print(\"Decoding bad time:      {}\".format(D_hyp_results[4]))\n",
    "            print('Encoding time:          {}'.format(Encode_time))\n",
    "            print(\"Decoding time:    {}\".format(D_hyp_decode_time))\n",
    "        \n",
    "        One_run_results = np.array([emb_loss,           # emb_loss\n",
    "                                    D_hyp_results[0],   # mean of tree loss \n",
    "                                    D_hyp_results[1],   # std of tree loss\n",
    "                                    D_hyp_results[2],   # best(min) of tree loss\n",
    "                                    D_hyp_results[4],   # times that decoding fail (inf distance)\n",
    "                                    Encode_time,        # Encoding time\n",
    "                                    D_hyp_decode_time]) # Decoding time\n",
    "        return One_run_results\n",
    "    \n",
    "    else:\n",
    "        # Encoding part (just generate/load data)\n",
    "        D_metric = train(args)\n",
    "\n",
    "        # Decoding part\n",
    "        if args.dec_method in ['TreeRep']:\n",
    "            start = time.time()\n",
    "            D_metric_results = Decoding_TreeRep(D_input=D_metric,D_target=D_metric,p=args.p,repeat=args.dec_repeat,verbose=False,rng_seed=args.seed)\n",
    "            D_metric_decode_time = (time.time()-start)/args.dec_repeat\n",
    "        elif args.dec_method in ['NJ']:\n",
    "            start = time.time()\n",
    "            D_metric_results = Decoding_NJ(D_input=D_metric,D_target=D_metric,p=args.p,verbose=False)\n",
    "            D_metric_decode_time = (time.time()-start)\n",
    "        elif args.dec_method in ['MST']:\n",
    "            start = time.time()\n",
    "            D_metric_results = Decoding_MST(D_input=D_metric,D_target=D_metric,p=args.p,verbose=False)\n",
    "            D_metric_decode_time = (time.time()-start)\n",
    "        elif args.dec_method in ['single','complete','average','weighted','centroid','median','ward']:\n",
    "            start = time.time()\n",
    "            D_metric_results = Decoding_linkage(D_input=D_metric,D_target=D_metric,p=args.p,\n",
    "                                                verbose=False,link_method=args.dec_method)\n",
    "            D_metric_decode_time = (time.time()-start)\n",
    "        else:\n",
    "            raise NameError('No such a decoding method! name={}'.format(args.dec_method))\n",
    "            \n",
    "        # Check if the output is a tree metric\n",
    "        if args.check_tree:\n",
    "            print(is_treemetric(D_metric_results[3]))\n",
    "        \n",
    "        # Results\n",
    "        if args.verbose:\n",
    "            print(\"Seed:{},p={},dataset:{},num_nodes={}\".format(args.seed,args.p,args.dataset,D_metric.shape[0]))\n",
    "            print(\"Enc:{},Dec:{}\".format(args.enc_method,args.dec_method))\n",
    "            print(\"Tree loss:              {}±{}, best loss:{}\".format(np.around(D_metric_results[0],4),np.around(D_metric_results[1],4),np.around(D_metric_results[2],4)))\n",
    "            print(\"Decoding bad time:      {}\".format(D_metric_results[4]))\n",
    "            print(\"Decoding time:    {}\".format(D_metric_decode_time))\n",
    "        \n",
    "        One_run_results = np.array([-1,                    # No emb_loss. Set to -1.\n",
    "                                    D_metric_results[0],   # mean of tree loss \n",
    "                                    D_metric_results[1],   # std of tree loss\n",
    "                                    D_metric_results[2],   # best(min) of tree loss\n",
    "                                    D_metric_results[4],   # times that decoding fail (inf distance)\n",
    "                                    -1,                    # No Encoding time. Set to -1. \n",
    "                                    D_metric_decode_time]) # Decoding time\n",
    "        return One_run_results\n",
    "    \n",
    "def All_Run_Exp(args,\n",
    "                DATASET_list=['zoo'],\n",
    "                NOISE_list=[1.0],\n",
    "                NUM_NODES_list=[16],\n",
    "                ENC_list=['Direct'],\n",
    "                DEC_list=['TreeRep'],\n",
    "                ENC_repeat=10):\n",
    "    \n",
    "    column_name = ['Dataset','Num nodes','Enc','Dec',\n",
    "                           'Emb loss','Mean','Std','Best','Bad time',\n",
    "                           'Enc time','Dec time']\n",
    "    \n",
    "    \n",
    "    Original_seed = args.seed\n",
    "    \n",
    "    for dname in DATASET_list:\n",
    "        args.dataset = dname\n",
    "        for n in NUM_NODES_list:\n",
    "            args.num_nodes = n\n",
    "            for noise in NOISE_list:\n",
    "                args.noise_scale = noise\n",
    "                All_results = pd.DataFrame(columns=column_name)\n",
    "                for enc in ENC_list:\n",
    "                    args.enc_method = enc\n",
    "                    for dec in DEC_list:\n",
    "                        args.dec_method = dec\n",
    "\n",
    "                        # Remember to set hyperparameters for each combinations\n",
    "                        if dname in ['zoo']:\n",
    "                            args.learning_rate = 0.05\n",
    "                            args.scaling_factor = 1.0\n",
    "                            args.rank = 64\n",
    "                        elif dname in ['iris']:\n",
    "                            args.learning_rate = 0.05\n",
    "                            args.scaling_factor = 1.0\n",
    "                            args.rank = 64\n",
    "                        elif dname in ['glass']:\n",
    "                            args.learning_rate = 0.05\n",
    "                            args.scaling_factor = 1.0\n",
    "                            args.burnin_factor = 50\n",
    "                            args.rank = 64\n",
    "                        elif dname in ['segmentation']:\n",
    "                            args.learning_rate = 0.02\n",
    "                            args.scaling_factor = 1.0\n",
    "                            args.rank = 64\n",
    "                            args.epochs = 100\n",
    "                            args.patience = 10\n",
    "                            args.burnin = 20\n",
    "                            args.burnin_factor = 2\n",
    "                            args.batch_size = 10000\n",
    "                            args.num_workers = 64\n",
    "                        elif dname in ['spambase']:\n",
    "                            args.learning_rate = 0.002\n",
    "                            args.scaling_factor = 1.0\n",
    "                            args.rank = 16\n",
    "                            args.epochs = 100\n",
    "                            args.patience = 5\n",
    "                            args.burnin = 5\n",
    "                            args.burnin_factor = 1\n",
    "                            args.batch_size = 100000\n",
    "                            args.num_workers = 64\n",
    "\n",
    "                        elif dname in ['random_tree']:\n",
    "                            if n in [64]:\n",
    "                                if noise in [0.1,0.3,0.5]:\n",
    "                                    args.learning_rate = 0.001\n",
    "                                    args.scaling_factor = 0.01\n",
    "                                    args.rank = 64\n",
    "                                    args.burnin = 50\n",
    "                                    args.burnin_factor = 10\n",
    "                                    args.batch_size = 1000000\n",
    "                                    args.num_workers = 1\n",
    "\n",
    "                            elif n in [128]:\n",
    "                                if noise in [0.1,0.3,0.5]:\n",
    "                                    args.learning_rate = 0.05\n",
    "                                    args.scaling_factor = 0.05\n",
    "                                    args.rank = 64\n",
    "                                    args.burnin = 50\n",
    "                                    args.burnin_factor = 10\n",
    "                                    args.batch_size = 1000000\n",
    "                                    args.num_workers = 1\n",
    "                            \n",
    "                            elif n in [256]:\n",
    "                                if noise in [0.1,0.3,0.5]:\n",
    "                                    args.learning_rate = 0.001\n",
    "                                    args.scaling_factor = 0.01\n",
    "                                    args.rank = 64\n",
    "                                    args.burnin = 30\n",
    "                                    args.burnin_factor = 10\n",
    "                                    args.batch_size = 1000000\n",
    "                                    args.num_workers = 1\n",
    "                        else:\n",
    "                            print('Use default settings given by args.')\n",
    "\n",
    "                        args.seed = Original_seed\n",
    "\n",
    "                        for _ in range(ENC_repeat):\n",
    "                            All_results = All_results.append(pd.Series(), ignore_index=True)\n",
    "                            All_results.iloc[-1,0] = dname\n",
    "                            All_results.iloc[-1,1] = n\n",
    "                            All_results.iloc[-1,2] = enc\n",
    "                            All_results.iloc[-1,3] = dec\n",
    "                            All_results.iloc[-1,4:] = One_Run_Exp(args)\n",
    "                            # remember to change args.seed\n",
    "                            args.seed += 1\n",
    "                        \n",
    "                if args.save:\n",
    "                    # Write to a csv file\n",
    "                    now = datetime.now()\n",
    "                    dt_string = now.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "                    if dname in ['zoo','iris','glass','segmentation','spambase','letter-recognition']:\n",
    "                        SAVE_PATH = './Results/All_{}_results_{}.csv'.format(dname,dt_string)\n",
    "                    else:\n",
    "                        SAVE_PATH = './Results/All_{}_{}_{}_results_{}.csv'.format(dname,n,noise,dt_string)\n",
    "                    All_results.to_csv(SAVE_PATH,index=False)\n",
    "                    # Also write the args\n",
    "                    with open(SAVE_PATH, 'a') as f:\n",
    "                        print('#',args, file=f)\n",
    "\n",
    "                    print('Results finished and saved at {}'.format(SAVE_PATH))\n",
    "                else:\n",
    "                    print(args)\n",
    "            \n",
    "    return All_results\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Run the experiment\n",
    "\n",
    "Note that if you use new datasets or other settings of synthetic datasets, you have to tune the hyperparameters at part 2 by your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_list choice: 'zoo', 'iris', 'glass', 'segmentation','spambase','random_tree'\n",
    "# DEC_list: 'NJ','TreeRep','single','complete','average','weighted','centroid','median', 'ward'\n",
    "\n",
    "All_Run_Exp(args,\n",
    "            DATASET_list=['zoo'], # List of datasets to be tested.\n",
    "            NUM_NODES_list=[64], # number of nodes for synthetic datasets, not used for real-world datasets.\n",
    "            NOISE_list=[0.3], # edge noise ratio, not used for real-world datasets.\n",
    "            ENC_list=['Direct','Hyp'], # 'Direct' for using decoder directly, 'Hyp' for HyperAid\n",
    "            DEC_list=['NJ'], # Choice of decoders.\n",
    "            ENC_repeat=1) # number of independent runs for encoder part. Set to 1 for real-world datasets and 3 for `random_tree`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyphc",
   "language": "python",
   "name": "hyphc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
